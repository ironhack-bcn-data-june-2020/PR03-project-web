{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "surprised-miniature",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from requests.exceptions import HTTPError, ConnectionError, SSLError, Timeout, TooManyRedirects\n",
    "from time import sleep\n",
    "from random import choice, randint\n",
    "\n",
    "class DEGIROScraper:\n",
    "\n",
    "    def __init__(self, url_pattern, sleep=True, content_parser=None):\n",
    "        \n",
    "        self.url_pattern = url_pattern #stocks, bonds, funds, or etfs\n",
    "        \n",
    "        self.sleep = sleep\n",
    "        self.content_parser = content_parser\n",
    "    \n",
    "    def scrape_urls(self, url): #Funcion que se aprovecha de la API interna de GIRO para extrar informaci√≥n de sus productos financieros\n",
    "        \n",
    "        total = 1\n",
    "        product_list = []\n",
    "\n",
    "        #Parametros de lo URL\n",
    "        params = {'requireTotal':'True', #Le solicitamos que nos devuelva un atributo con el total de resultado  \n",
    "        'offset': 0 ,                    #Punto de donde empieza la busqueda\n",
    "        'limit':'1000',                  #Resultados totales por get (1000 es el maximo)\n",
    "        'intAccount':'55010998',         #Valor asociado a mi cuenta de DEGIRO (es siempre el mismo)\n",
    "        'sessionId': '693E145AFA74299FE0E1C8DD65ABE1FB.prod_b_128_4'} #Valor asociado a cada sesion que se inicia en DEGIRO\n",
    "                                                                        #Cambia con cada login\n",
    "        \n",
    "        headers = {'user-agent' : self.get_random_useragent(),  #Funcion para obtener un user-agent al azar\n",
    "                  'refer': self.get_referer(),                  #Funcion para obtener un referer segun el producto financiero\n",
    "                  'accept': 'application/json, text/plain, */*', #Rellenamos estos requests headers que suele pedir DEGIRO,\n",
    "                  'accept-encoding':'gzip, deflate, br',           #gracias a esto es mas facil disimular nuestra procedencia.\n",
    "                  'accept-language':'en-US,en;q=0.9,ca;q=0.8,es;q=0.7,pt;q=0.6'}                  \n",
    "        \n",
    "        while params['offset'] < total: #Se usa el offset como el indice del while total se define mas adelante.\n",
    "            \n",
    "            try: #Creamos un try,except por si se produce algun error con el get\n",
    "                \n",
    "                response = requests.get(url, headers=headers, params=params) #Peticion get con parametros y headers definidos\n",
    "\n",
    "                if response.status_code != 200: #Si el codigo no es 200 que se cree una exception\n",
    "                    print(response.status_code)\n",
    "                    raise Exception('Status Code Not Succesful')\n",
    "\n",
    "                result = response.json() #Convertimos la respuesta en un diccionario\n",
    "\n",
    "            except (HTTPError, ConnectionError, SSLError, Timeout, TooManyRedirects) as err: #Atrapamos estos errores si se producen\n",
    "                print(err)\n",
    "\n",
    "            except Exception as err:\n",
    "                print(err)\n",
    "            finally:\n",
    "                              \n",
    "                total = result['total']  #Total siempre sera igual al maximo de resultados \n",
    "                product_list.extend(result['products']) #Extendemos la lista de los resusltados obtenidos con los de la siguiente iteracion \n",
    "                params['offset'] += 1000 #Sumamos 1000 a nuestro indice\n",
    "\n",
    "                self.sleep_interval() #Intervalo de espera al azar.\n",
    "\n",
    "\n",
    "        self.output_results(product_list) #Exportamos el Dataset obtenido\n",
    "\n",
    "    def sleep_interval(self):   #Funcion que generar tiempos de esperar al azar entre GETs\n",
    "        if self.sleep == True:\n",
    "            delays = [7, 4, 6, 2, 10, 19]    \n",
    "            sleep(choice(delays))\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    def get_random_useragent(self):  #Funcion que se encarga de generar user-agents al azar a partir de un txt\n",
    "        with open('user-agent.txt') as f:\n",
    "            lines = f.readlines()\n",
    "            \n",
    "        return lines[randint(0,len(lines)-1)].strip()\n",
    "    \n",
    "    def get_referer(self): #Selecciona el referer segun el tipo de scrapeo que se este haciendo\n",
    "        referer = {'stocks':'https://trader.degiro.nl/beta-trader/#/products?productType=1',    \n",
    "                'bonds':'https://trader.degiro.nl/beta-trader/#/products?productType=2',\n",
    "                'funds':'https://trader.degiro.nl/beta-trader/#/products?productType=13',\n",
    "                'etfs':'https://trader.degiro.nl/beta-trader/#/products?productType=131'}\n",
    "    \n",
    "        return referer[self.url_pattern]\n",
    "    \n",
    "    \n",
    "    def output_results(self, product_list): #Funcion que exporta el resultado obtenido a un archivo .csv\n",
    "        \n",
    "        pd.DataFrame(product_list).to_csv( self.url_pattern + '.csv', sep=',', index=False)    \n",
    "\n",
    "\n",
    "    def kickstart(self): #Funcion inicilizadora del proceso de scrapeo\n",
    "\n",
    "        url = 'https://trader.degiro.nl/product_search/secure/v5/%s?'  #URL base de DEGIRO para la busqueda de productos\n",
    "        self.scrape_urls(url%self.url_pattern)                         #En %s se introduce el producto a buscar: bonds,funds,stocks or etfs\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "checked-literature",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bonds = DEGIROScraper(url_pattern='bonds', sleep=True)\n",
    "Bonds.kickstart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "educational-visit",
   "metadata": {},
   "outputs": [],
   "source": [
    "Funds = DEGIROScraper(url_pattern='funds', sleep=True)\n",
    "Funds.kickstart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "played-hello",
   "metadata": {},
   "outputs": [],
   "source": [
    "ETFs = DEGIROScraper(url_pattern='etfs', sleep=True)\n",
    "ETFs.kickstart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "champion-protest",
   "metadata": {},
   "outputs": [],
   "source": [
    "Stocks = DEGIROScraper(url_pattern='stocks', sleep=True)\n",
    "Stocks.kickstart()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
